<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Role of Better visual representation for Long Horizon Tasks">
  <meta name="keywords" content="Vision-Language Model, VAE, Long Horizon Tasks">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Role of Better visual representation for Long Horizon Tasks</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>


  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Behavior Transformers with better visual representation for Long Horizon Tasks</h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                Yaswanth Orru,
                Chenyu Wang

            </div>


          <div class="column has-text-centered">
              </span>
              <!-- Video Link. -->
              <!-- <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/WCY2000/DDRL_project"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>

                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://osf.io/983qz/"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/videos/teaser.mp4"
                type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        <span class="dnerf">HiSS</span> turns selfie videos from your phone into
        free-viewpoint
        portraits.
      </h2>
    </div>
  </div>
</section> -->

<!--
<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-steve">
          <video poster="" id="steve" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/steve.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-chair-tp">
          <video poster="" id="chair-tp" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/chair-tp.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-shiba">
          <video poster="" id="shiba" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/shiba.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-fullbody">
          <video poster="" id="fullbody" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/fullbody.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-blueshirt">
          <video poster="" id="blueshirt" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/blueshirt.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-mask">
          <video poster="" id="mask" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/mask.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-coffee">
          <video poster="" id="coffee" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/coffee.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-toby">
          <video poster="" id="toby" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/toby2.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> -->



<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            The introduction of the Behavior Transformer (BeT) <sup>[1]</sup> started to address the challenge of leveraging large, unlabeled, human-generated datasets for Offline Reinforcement Learning (RL) and Behavioral Cloning. It utilizes a transformative approach by incorporating action discretization and multi-task action correction techniques into standard transformer architectures, enabling effective modeling of complex, multi-modal human behaviors. BeT has demonstrated superior performance compared to other models in the Relay Kitchen environment, utilizing robot and object state observations. This success prompts us to explore its potential in scenarios involving partial observations, such as image sequences, which are more commonly encountered in real-life settings.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video. -->
    <!-- <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
          <source src="./static/videos/hiss_video.mp4"
                  type="video/mp4">
        </video>

      </div>
    </div> -->
    <!--/ Paper video. -->
  </div>
</section>



<section class="section">

  <div class="container is-max-desktop">
          <!-- VLM-BeT -->
          <div class="columns is-centered">
            <div class="column is-full-width">
              <!-- <h2 class="title is-3">VLM-BeT: Behavior Transformers with Vision-Language Models</h2> -->

              <!-- Model Architecture. -->
              <h2 class="title is-2 has-text-centered">Model Pipeline</h2>
              <img src="./static/images/pipeline.png" alt="VLM-BeT Model Architecture">
              <div class="content has-text-centered">
                <!-- <p>
                  (Left) Flat SSM directly maps a sensor sequence to an output sequence.
                </p>
                <p>
                  (Right) <span class="dnerf">HiSS</span> divides an input sequence into chunks which are processed into <i>chunk features</i> by a low-level SSM.
                  A high-level SSM maps the resulting sequence to an output sequence.
                </p> -->
              </div>
              <div class="content has-text-centered">
              </div>
              <!--/ Model Architecture. -->

              <!-- BeT. -->
              <h2 class="title is-2 has-text-centered">BeT: Behavior Transformer</h2>
              <img src="./static/images/bet_architecture.png" alt="BeT model">
              <div class="content has-text-lefted">
                <p>
                  BeT is a new method for learning behaviors from rich, distributionally multi-modal data. It clusters continuous actions into discrete bins using k-means clustering, simplifying the representation of high-dimensional action spaces as categorical distributions.
                </p>

              </div>
              <div class="content has-text-lefted">
              </div>
              <!--/ BeT. -->

              <!-- Encoder Selection. -->
              <h2 class="title is-2 has-text-centered">Encoder Selection</h2>
              <!-- <img src="./static/images/result_table.png" alt="VLM-BeT Model Results"> -->
              <div class="content has-text-lefted">
                <h3 class="title is-4">Vision-Language Models: Language Segment-Anything (Lang-Sam) <sup>[2]</sup></h3>
                <p>
                  We employ distinct prompts for each task to extract latent variables that serve as image embeddings. This technique allows us to tailor the model's input to the specific characteristics of each task, enhancing the relevance and accuracy of the generated embeddings.
                </p>
                <h3 class="title is-4">Variational Autoencoder: Disentangled Inferred Prior VAE (DIP-VAE) <sup>[3]</sup></h3>
                <p>
                  We trained the model using all images extracted from the training set videos, employing the latent variables produced by the encoder as image embeddings. Building upon the foundational architecture of the original VAE, our approach incorporates the DIP-VAE model, which introduces an additional regularization term in the loss function. This term is designed to encourage the latent variables to exhibit statistical independence, thus promoting a disentangled representation that enhances interpretability and usability in downstream tasks. The output dimension of DIP-VAE: [32,64,128,256]
                </p>
                <img src="./static/images/DIPVAE_reconstruction.png" alt="DIPVAE reconstruction">
                <p>The figure shows the original image and reconstructed image from DIP-VAE on the validation set randomly selected from the videos.</p>
                <h3 class="title is-4"> Pretrained ResNet18  <sup>[4]</sup></h3>
                <p>
                  We tried pre-trained ResNet 18 to generate image embeddings with an output dimension of 512.
                </p>

              </div>
              <div class="content has-text-lefted">
              </div>
              <!--/ Encoder Selection. -->


              <!-- Experiments and Results. -->
              <h2 class="title is-2 has-text-centered">Experiments and Results</h2>
              <!-- <img src="./static/images/result_table.png" alt="VLM-BeT Model Results"> -->
              <div class="content has-text-lefted">
                <h3 class="title is-4">Franka kitchen environment</h3>
                <p>
                  To highlight the complexity of performing long sequences of actions, we use the Relay Kitchen Environment where a Franka robot manipulates a virtual kitchen environment. We use the relay policy learning dataset with 566 demonstrations collected by human participants wearing VR headsets. The participants completed a sequence of four object-interaction tasks in each episode . There are a total of seven interactable objects in the kitchen: a microwave, a kettle, a slide cabinet, a hinge cabinet, a light switch, and two burner knobs. This dataset contains two different kinds of multi-modality: one from the inherent noise in human demonstrations, and another from the demonstrators' intent.
                </p>
                <div class="columns">
                  <div class="column is-half">
                    <!-- Video -->
                    <video id="dollyzoom" autoplay controls muted loop playsinline width="100%">
                      <source src="./static/videos/000_view0.mp4" type="video/mp4">
                    </video>
                  </div>
                  <div class="column is-half">
                    <!-- Image -->
                    <img src="./static/images/demo_freq.png" alt="Datasets">
                  </div>
                </div>
                <h3 class="title is-4">Rollout Results</h3>
                <p>
                  We compare the performance of BeT with different encoders in learning from demonstrations. We measure the probability of n tasks being completed by the model within the allotted 280 timesteps. Evaluations are over 500 rollouts for all the BeT with encoders.
                </p>
                <style>
                  table {
                      width: 100%;
                      border-collapse: collapse;
                  }
                  th, td {
                      border-left: 1px solid #ddd; /* Add vertical lines */
                      padding: 8px; /* Padding for aesthetic spacing inside cells */
                      text-align: center; /* Center text horizontally in each cell */
                  }
                  th:first-child, td:first-child {
                      border-left: none; /* Remove border from the first cell in each row */
                  }
              </style>
              <table border="1">
                  <tr>
                      <th></th>
                      <th>1</th>
                      <th>2</th>
                      <th>3</th>
                      <th>4</th>
                      <th>5</th>
                  </tr>
                  <tr>
                      <td><strong>Original BeT</strong></td>
                      <td>0.99</td>
                      <td>0.93</td>
                      <td>0.71</td>
                      <td>0.44</td>
                      <td>0.02</td>
                  </tr>
                  <tr>
                      <td><strong>Resnet18 + BeT</strong></td>
                      <td>0.71</td>
                      <td>0.336</td>
                      <td>0.138</td>
                      <td>0.016</td>
                      <td>0</td>
                  </tr>
                  <tr>
                      <td><strong>DIPVAE (dim = 32)+ BeT</strong></td>
                      <td>0.826</td>
                      <td>0.294</td>
                      <td>0.07</td>
                      <td>0.004</td>
                      <td>0</td>
                  </tr>
                  <tr>
                      <td><strong>DIPVAE (dim = 64) + BeT</strong></td>
                      <td>0.944</td>
                      <td>0.682</td>
                      <td>0.436</td>
                      <td>0.152</td>
                      <td>0.02</td>
                  </tr>
                  <tr>
                      <td><strong>DIPVAE (dim = 128) + BeT</strong></td>
                      <td>0.986</td>
                      <td>0.738</td>
                      <td>0.502</td>
                      <td>0.184</td>
                      <td>0.014</td>
                  </tr>
                  <tr>
                      <td><strong>DIPVAE (dim = 256) + BeT</strong></td>
                      <td>0.978</td>
                      <td>0.744</td>
                      <td>0.53</td>
                      <td>0.126</td>
                      <td>0.012</td>
                  </tr>
              </table>


                <h3 class="title is-4">Training Results</h3>
                We trained the BeT models for 50 epochs with the default hyperparameters with different encoders.
                <table border="1" style="width: 100%; border-collapse: collapse;">
                  <tr>
                      <th>Experiment Name</th>
                      <th>Encoder Model</th>
                      <th>Observation Dimension</th>
                      <th>Dataset Size</th>
                  </tr>
                  <tr>
                      <td>original_BeT</td>
                      <td>/</td>
                      <td>60 (only 30 is non-zero variables)</td>
                      <td>566</td>
                  </tr>
                  <tr>
                      <td>resnet18_512</td>
                      <td>ResNet 18</td>
                      <td>512</td>
                      <td>566</td>
                  </tr>
                  <tr>
                      <td>dipvae_32</td>
                      <td>DIP-VAE</td>
                      <td>32</td>
                      <td>566</td>
                  </tr>
                  <tr>
                      <td>dipvae_64</td>
                      <td>DIP-VAE</td>
                      <td>64</td>
                      <td>566</td>
                  </tr>
                  <tr>
                      <td>dipvae_128</td>
                      <td>DIP-VAE</td>
                      <td>128</td>
                      <td>566</td>
                  </tr>
                  <tr>
                      <td>dipvae_256</td>
                      <td>DIP-VAE</td>
                      <td>256</td>
                      <td>566</td>
                  </tr>
                  <tr>
                      <td>vlm_ds_32</td>
                      <td>Lang-SAM</td>
                      <td>1048576</td>
                      <td>32</td>
                  </tr>
                  <tr>
                      <td>vlm_ds_100</td>
                      <td>Lang-SAM</td>
                      <td>1048576</td>
                      <td>100</td>
                  </tr>
              </table>

                <img src="./static/images/train.png" alt="training plot">
                <img src="./static/images/val.png" alt="validation plot">



              </div>
              <div class="content has-text-lefted">
              </div>
              <!--/ Experiments and Results. -->

              <!-- Model Architecture. -->
              <h2 class="title is-2 has-text-centered">Conclusion</h2>

              <div class="content has-text-lefted">
                <p>
                  In our project, we experimented with various encoder architectures fore image embdding, including VLM, VAE, and ResNet. Notably, DIP-VAE outperformed ResNet during rollout scenarios. This superior performance can be attributed to the DIP-VAE being trained directly on images from the training set videos, unlike the pre-trained ResNet-18, which was trained on the CIFAR dataset primarily for classification tasks. Consequently, ResNet-18 was less effective in accurately representing the specific observations required for our application.
                </p>
                <p>
                  Further observations revealed that although ResNet exhibited similar performance to DIP-VAE on the validation set, its performance significantly deteriorated during rollout. This discrepancy likely arises because the training set videos encompass a limited variety of states, whereas rollout scenarios introduce a broader range of states, including many that are unseen during training.
                </p>

              </div>
              <div class="content has-text-lefted">
              </div>
              <!--/ Model Architecture. -->
            </div>
          </div>
          <!--/ VLM-BeT -->

  </div>
</section>


<!-- <section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{park2021nerfies,
  author    = {Park, Keunhong and Sinha, Utkarsh and Barron, Jonathan T. and Bouaziz, Sofien and Goldman, Dan B and Seitz, Steven M. and Martin-Brualla, Ricardo},
  title     = {Nerfies: Deformable Neural Radiance Fields},
  journal   = {ICCV},
  year      = {2021},
}</code></pre>
  </div>
</section> -->


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">

      <!-- <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a> -->
      <!-- <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a> -->
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p style="text-align: left;">
            <sup>[1]</sup> Shafiullah, N. M. M., Cui, Z. J., Altanzaya, A., and Pinto, L. Behavior Transformers: Cloning k modes with one stone. Neural Information Processing Systems, 2022.
          </p>
          <p style="text-align: left;">
            <sup>[2]</sup> https://github.com/paulguerrero/lang-sam
          </p>
          <p style="text-align: left;">
            <sup>[3]</sup> Kumar, A., Sattigeri, P., and Balakrishnan, A. Variational Inference of Disentangled Latent Concepts from Unlabeled Observations. International Conference on Learning Representations, 2018.
          </p>
          <p style="text-align: left;">
            <sup>[4]</sup> https://www.tensorflow.org/tfmodels/vision/image_classification
          </p>
          <p style="text-align: left;">
            <sup>[5]</sup> Li, J., Li, D., Xiong, C., and Hoi, S. C. H. BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation. International Conference on Machine Learning, 2022
          </p>
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
